<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN">
<!-- saved from url=(0037)http://www.stat.ucla.edu/~pwei/ --><HTML
xml:lang="en"
xmlns="http://www.w3.org/1999/xhtml"><HEAD><TITLE>ProjectHOI</TITLE>
<META content="text/html; charset=UTF-8" http-equiv=Content-Type>

<META content=IE=Edge,chrome=1 http-equiv=X-UA-Compatible>

<STYLE type=text/css>
@media Print
{
#STTBimg {
    DISPLAY: none
}
#STTBimg2 {
    DISPLAY: none
}
    }
</STYLE>
<style type="text/css">
        body{background:#FFFFFF;text-align:center;}
        div{width:778px;margin:0 auto;background:#fff;text-align:left;}
.STYLE3 {color: #FF0000}
</style>


<META name=GENERATOR content="MSHTML 9.00.8112.16441"></HEAD>
<BODY class=" hasGoogleVoiceExt" screen_capture_injected="true">
<DIV id=top></DIV>
<DIV id=content>
      <DIV id=wikitext>
      <H1 style="TEXT-ALIGN: center">Exemplar-based Recognition of<br>
      Human-Object Interactions</H1>
      <P style="TEXT-ALIGN: center"><A class=urllink title=""
      href="HomePage.htm"
      rel=nofollow>Jian-Fang Hu</A><SUP>1</SUP>, <A class=urllink title=""
      href="http://sist.sysu.edu.cn/~zhwshi/" rel=nofollow>Wei-Shi Zheng</A><SUP>1</SUP>, <a href="http://sist.sysu.edu.cn/main/default/teainfo.aspx?id=45&no=1&pId=10">Jianhuang Lai</a><SUP>1</SUP>, <a href="http://www.eecs.qmul.ac.uk/~sgg/">Shaogang Gong</a><SUP>2</SUP>, and <a href="http://www.eecs.qmul.ac.uk/~txiang/">Tao Xiang</a><SUP>2</SUP></P>
      <P style="TEXT-ALIGN: center" class=vspace><SUP>1</SUP>Sun Yat-sen University, China&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<SUP>2</SUP>Queen Mary, University of London</P>
      <DIV class=vspace></DIV>
      <H2>Introduction</H2>
      <P>Human action can be recognised from a single still image by modelling human-object interactions (HOI), which infers the mutual spatial structure information between human and the manipulated object as well as their appearance. Existing approaches rely heavily on accurate detection of human and object and estimation of human pose; they are thus sensitive to large variations of human poses, occlusion and unsatisfactory detection of small size objects. To overcome this limitation, a novel exemplar-based approach is proposed in this work. Our approach learns a set of spatial pose-object interaction exemplars, which are probabilistic density functions describing spatially how a person is interacting with a manipulated object for different activities. Specifically, a new framework consisting of an exemplar-based HOI descriptor and an associated matching model is formulated for robust human action recognition in still images. In addition, the framework is extended to perform HOI recognition in videos, where the proposed exemplar representation is used for implicit frame selection to negate irrelevant or noisy frames by temporal structured HOI modelling. Extensive experiments are carried out on two image action datasets and two video action datasets. The results demonstrate the effectiveness of our proposed methods and show that our approach is able to achieve state-of-the-art performance, compared with several recently proposed competitors.</P>
      <DIV class=vspace></DIV>
      <DIV style="TEXT-ALIGN: center"></DIV>
      <DIV class=vspace></DIV>
      <img src="imageFiles/HOITCSVT.jpg" width="800" height="350">
      <H2>&nbsp;</H2>
      <DIV class=vspace></DIV>
      <H2>Download</H2>
      <P>
Please download the dataset and features using the links below:       
      <P>Sports Image set (300 HOI images): <a href="features/PHOW_CSVT.zip">PHOW features &amp; Label Info</a>
      <P><a href="http://ai.stanford.edu/~bangpeng/ppmi.html">PPMI Image set</a> (2400 HOI images): <a href="features/PHOW_CSVT.zip">PHOW features, Label Info</a>, Annotations
      (<span class="STYLE3">coming soon</span>)
      <P><a href="http://www.cs.cmu.edu/~abhinavg/">CMU-Gupta Dataset</a> (54 HOI video clips): <a href="datasets/A.Gupata.rar">Image sequences  </a><a href="datasets/SYSUDataset.rar">&amp;</a><a href="datasets/A.Gupata.rar"> Annotations</a>
      <P>SYSU Dataset (119 HOI video clips): <a href="datasets/SYSUDataset.rar">VideoData &amp; Annotations
      </a>
      
      <h2>Citation      </h2>
      <P>Please cite the following papers if you use the data in your research:
      <ul>        
      <LI> Jian-Fang Hu, Wei-Shi Zheng, Jianhuang Lai, Shaogang Gong, and Tao Xiang, “Recognising Human-Object
      Interaction via Exemplar based Modelling”, ICCV, 2013. Darling Harbour, Sydney.  <a href="pdfFiles/iccvfinal.pdf">PDF</a> <a href="Bib/ICCV2013.txt">BibTex</a> </Li>
         <LI> Jian-Fang Hu, Wei-Shi Zheng, Jianhuang Lai, Shaogang Gong, and Tao Xiang, “Exemplar-based Recognition
         of Human-Object Interactions”, IEEE trans on Circuits and Systems for Video Technology (TCSVT),  <a href="pdfFiles/iccvfinal.pdf"></a>2015. <a href="pdfFiles/iccvfinal.pdf">PDF</a> <a href="Bib/TCSVT2015.txt">BibTex</a></Li>
         <ul>
      </P>
      <DIV class=vspace></DIV>
      </P></TD></TR></TBODY></TABLE><!--PageFooterFmt-->
<DIV id=wikifoot>
</DIV></BODY></HTML>
